{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48de9623",
   "metadata": {},
   "source": [
    "<img src=\"./lecture_image/00_title.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6130b7d",
   "metadata": {},
   "source": [
    "<table border=1 width=100%>\n",
    "    <tr><td style=\"border: 1px solid black; width:600px; height:30px; text-align: center;\"><font size=4 color=blue><b>[21차시] 학습목표</b></font></td></tr>       \n",
    "    <tr><td style=\"border: 1px solid black; text-align: left;\"><font size=3>\n",
    "        \n",
    "○ Mediapipe 라이브러리를 활용할 수 있다 <br><br>\n",
    "○ Mediapipe 라이브러리를 이용하여 얼굴 특성을 추출할 수 있다 <br><br>\n",
    "○ Mediapipe 라이브러리를 이용하여 동작 특성을 추출할 수 있다 </font></td></tr>   \n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301e97a",
   "metadata": {},
   "source": [
    "# Mediapipe 라이브러리를 이용한 얼굴 3D 데이터 추출"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e72c546",
   "metadata": {},
   "source": [
    "- 라이브 및 스트리밍 미디어를 위한 플랫폼 간 사용자 지정 가능한 ML 솔루션을 제공\n",
    "- mediapipe 라이브러리 : https://google.github.io/mediapipe/getting_started/python\n",
    "- 기능\n",
    "<table width=800>\n",
    "    <tr>\n",
    "        <td><center><b>얼굴 검출</b></center></td>\n",
    "        <td><center><b>얼굴 Mesh</b></center></td>\n",
    "        <td><center><b>홍채 검출</b></center></td>\n",
    "        <td><center><b>손동작 검출</b></center></td>\n",
    "        <td><center><b>동작 검출</b></center></td>\n",
    "        <td><center><b>전체동작</b></center></td>\n",
    "    </tr>     \n",
    "    <tr>\n",
    "        <td><img src=\"./lecture_image/21_mediapipe_face_detection.gif\"></td>\n",
    "        <td><img src=\"./lecture_image/21_mediapipe_face_mesh.gif\"></td>\n",
    "        <td><img src=\"./lecture_image/21_mediapipe_iris_tracking.gif \"></td>\n",
    "        <td><img src=\"./lecture_image/21_mediapipe_hand_tracking.gif\"></td>\n",
    "        <td><img src=\"./lecture_image/21_mediapipe_pose_tracking.gif\"></td>\n",
    "        <td><img src=\"./lecture_image/21_mediapipe_holistic_tracking.gif\"></td>\n",
    "    </tr>    \n",
    "    <tr>\n",
    "        <td><center><b>헤어 분리</b></center></td>\n",
    "        <td><center><b>객체 검출</b></center></td>\n",
    "        <td><center><b>박스 추적</b></center></td>\n",
    "        <td><center><b>모션 추적</b></center></td>\n",
    "        <td><center><b>3D객체 검출</b></center></td>\n",
    "        <td><center><b>템플릿 매칭</b></center></td>\n",
    "    </tr> \n",
    "    <tr>\n",
    "        <td><img src=\"./lecture_image/21_mediapipe_hair_segmentation.gif\"></td>        \n",
    "        <td><img src=\"./lecture_image/21_mediapipe_object_detection.gif\"></td>\n",
    "        <td><img src=\"./lecture_image/21_mediapipe_box_tracking.gif\"></td>\n",
    "        <td><img src=\"./lecture_image/21_mediapipe_instant_motion_tracking.gif\"></td>\n",
    "        <td><img src=\"./lecture_image/21_mediapipe_objectron_chair.gif\"></td>\n",
    "        <td><img src=\"./lecture_image/21_mediapipe_template_matching.gif\"></td>\n",
    "    </tr> \n",
    "</table>    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ff86e202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting mediapipe\n",
      "  Downloading mediapipe-0.8.11-cp37-cp37m-win_amd64.whl (49.0 MB)\n",
      "     ---------------------------------------- 49.0/49.0 MB 9.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from mediapipe) (3.5.2)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from mediapipe) (4.6.0.66)\n",
      "Requirement already satisfied: absl-py in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from mediapipe) (1.2.0)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from mediapipe) (21.4.0)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from mediapipe) (3.19.5)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from mediapipe) (1.21.5)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from matplotlib->mediapipe) (21.3)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from matplotlib->mediapipe) (9.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from matplotlib->mediapipe) (1.4.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from kiwisolver>=1.0.1->matplotlib->mediapipe) (4.3.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\envs\\opencv\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Installing collected packages: mediapipe\n",
      "Successfully installed mediapipe-0.8.11\n"
     ]
    }
   ],
   "source": [
    "# 프롬프트 창에서 설치\n",
    "!pip install mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ca08ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# No module named 'mediapipe'가 뜨는 경우\n",
    "#   - 설치 폴더 (아래 설치내용 확인) 확인해서 .\\anaconda3\\envs\\opencv\\lib\\site-packages로 이동\n",
    "\n",
    "# No module named 'mediapipe.python._framework_bindings가 뜨는 경우\n",
    "#   - "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587a70b8",
   "metadata": {},
   "source": [
    "## 얼굴 메시 추출"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "513aefcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# 결과를 시각화하는 라이브러리\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "# 얼굴 메시를 추출하는 라이브러리 가져오기\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "\n",
    "# 시각화 설정 (선두께, 원의 반지름)\n",
    "drawing_spec = mp_drawing.DrawingSpec(thickness=1, circle_radius=1)\n",
    "\n",
    "# 영상 불러오기\n",
    "cap = cv2.VideoCapture(\"./image/face3.mp4\")\n",
    "\n",
    "# 얼굴 메시 추출 (최소 검출 정확도, 최소 추적 정확도)\n",
    "with mp_face_mesh.FaceMesh(min_detection_confidence=0.5,\n",
    "                           min_tracking_confidence=0.5) as face_mesh :\n",
    "    while cap.isOpened() :\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret :\n",
    "            break\n",
    "        \n",
    "        # 이미지 쓰기 불가 설정 (속도)\n",
    "        frame.flags.writeable = False\n",
    "        \n",
    "        # 메시 검출 (랜드마크 검출)\n",
    "        results = face_mesh.process(frame)\n",
    "        \n",
    "        frame.flags.writeable = True\n",
    "        \n",
    "        # 랜드마크를 서로 연결 (매시)\n",
    "        # 랜드마크 점이 검출되었다면\n",
    "        if results.multi_face_landmarks :\n",
    "            # 검출 랜드마크를 하나씩 가져온다\n",
    "            for face_landmark in results.multi_face_landmarks :\n",
    "                # 랜드마크들끼리 선으로 연결\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=frame,\n",
    "                    landmark_list=face_landmark,\n",
    "                    # 얼굴 전체\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    # 그리기 설정\n",
    "                    landmark_drawing_spec=drawing_spec,\n",
    "                    connection_drawing_spec=drawing_spec                    \n",
    "                )\n",
    "        cv2.imshow(\"face mesh\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(33)\n",
    "        \n",
    "        if key == 49 :\n",
    "            break\n",
    "            \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda9efaa",
   "metadata": {},
   "source": [
    "## 얼굴, 왼손, 오른손, 동작 검출 (스켈레톤)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f17cf7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "\n",
    "# 그리기 기능\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "# 스켈레톤 추출 기능 (전체) - 양손, 양손가락, 얼굴, 몸통, 다리, 발\n",
    "mp_holistic = mp.solutions.holistic\n",
    "\n",
    "cap = cv2.VideoCapture(\"./image/face4.mp4\")\n",
    "\n",
    "# 랜드마크 점 찍기\n",
    "drawing_spec1 = mp_drawing.DrawingSpec(thickness=1, color=(0, 0, 255))\n",
    "# 선 그리기 \n",
    "drawing_spec2 = mp_drawing.DrawingSpec(thickness=3, color=(255, 0, 0))\n",
    "\n",
    "with mp_holistic.Holistic(min_detection_confidence=0.5,\n",
    "                          min_tracking_confidence=0.5) as holistic :\n",
    "    while cap.isOpened() :\n",
    "        ret, frame = cap.read()\n",
    "        \n",
    "        if not ret :\n",
    "            break\n",
    "            \n",
    "        frame.flags.writeable = False\n",
    "        results = holistic.process(frame)\n",
    "        frame.flags.writeable = True\n",
    "        \n",
    "        # 몸통/다리/발 부분의 스켈레톤 그리기\n",
    "        mp_drawing.draw_landmarks(frame,\n",
    "                                 results.pose_landmarks,\n",
    "                                 mp_holistic.POSE_CONNECTIONS,\n",
    "                                 landmark_drawing_spec=drawing_spec1,\n",
    "                                 connection_drawing_spec=drawing_spec2)\n",
    "        \n",
    "        cv2.imshow(\"skeleton\", frame)\n",
    "        \n",
    "        key = cv2.waitKey(33)\n",
    "        \n",
    "        if key ==  49 :\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23e4a1e",
   "metadata": {},
   "source": [
    "# 응용 : 자세 분석\n",
    "- https://github.com/spmallick/learnopencv/tree/master/Posture-analysis-system-using-MediaPipe-Pose\n",
    "\n",
    "- WorkFlow\n",
    "  - 각도를 확인할 관심 지점(랜드마크) 검색\n",
    "  - 표준 샘플 이미지(옆면 이미지)에 대한 분석 수행\n",
    "  - 좋은 자세와 나쁜 자세에 대한 임계값 범위를 검색\n",
    "  - 영상에 적용하기\n",
    "  \n",
    "<img src=\"./lecture_image/21_MediaPipe-pose-application.png\" width=70%>  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ce10223",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2524759d",
   "metadata": {},
   "source": [
    "- 오프셋 거리 계산\n",
    "  - 측면보기 상태에서 두 포인트 (눈, 어깨, 골반) 사이의 거리를 계산\n",
    "  \n",
    "\\begin{align}\n",
    "distance =  \\sqrt{(x2 - x1)^2+(y2 - y1)^2}\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b0004b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6514c61",
   "metadata": {},
   "source": [
    "- 관심 선에서 y축으로 종속된 각도 계산\n",
    "\n",
    "<img src=\"./lecture_image/21_calculate_arc.jpg\" width=40%>\n",
    "\n",
    "\\begin{align}\n",
    "\\theta = \\arccos (\\frac{\\vec{P_{12}}.\\vec{P_{13}}}{|\\vec{P_{12}}|.|\\vec{P_{13}}|})\n",
    "= \\arccos (\\frac{y_1^2 - y_1.y_2}{y_1\\sqrt{(x_2 - x_1)^2 + (y_2 - y_1)^2}})\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "374220f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ad8af4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#실습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85307ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing..\n",
      "Finished.\n"
     ]
    }
   ],
   "source": [
    "#실습"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b18b757",
   "metadata": {},
   "source": [
    "# 응용 : Human Action Recognition using Detectron2 and LSTM\n",
    "  - https://learnopencv.com/human-action-recognition-using-detectron2-and-lstm/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0e4535",
   "metadata": {},
   "source": [
    "<table border=1 width=100%>\n",
    "    <tr><td style=\"border: 1px solid black; width:600px; height:40px; text-align: center;\"><font size=4 color=blue><b>[21차시] 정리하기</b></font></td></tr>       \n",
    "    <tr><td style=\"border: 1px solid black; text-align: left;\"><font size=3>○            \n",
    "        </font></td></tr>   \n",
    "</table>"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
